import streamlit as st
from playwright.async_api import async_playwright # Mudamos para async_api
from fake_useragent import UserAgent
import pandas as pd
import asyncio
import random
import sys

# --- CONFIGURA√á√ÉO INICIAL (CR√çTICO PARA WINDOWS) ---
if sys.platform == 'win32':
    # Garante que o Windows use o loop que permite abrir subprocessos (navegador)
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

st.set_page_config(page_title="Scraper Pro Async", layout="wide")
st.title("üï∑Ô∏è Web Scraper App (Async Mode)")
st.markdown("---")

# --- 1. SCHEDULER ---
def scheduler(url_input):
    if not url_input:
        return []
    return [url.strip() for url in url_input.replace(',', '\n').split('\n') if url.strip()]

# --- 2. DOWNLOADER (Agora √© ASYNC) ---
# --- 2. DOWNLOADER (ATUALIZADO PARA ESPERAR A TABELA) ---
async def downloader_async(url):
    ua = UserAgent()
    user_agent = ua.random
    
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True) # Mude para False se quiser ver acontecendo
            
            context = await browser.new_context(user_agent=user_agent)
            page = await context.new_page()
            
            # Acessa a p√°gina
            await page.goto(url, timeout=60000, wait_until="domcontentloaded")
            
            # --- O PULO DO GATO ---
            # Esperamos aparecer um elemento espec√≠fico da tabela de odds.
            # Se n√£o esperar, ele pega o HTML antes da tabela carregar.
            try:
                # Tenta esperar por algo que pare√ßa uma c√©lula de aposta ou linha
                # DICA: Ajuste esse seletor se der Timeout. Tente algo gen√©rico como 'div' ou 'table'
                await page.wait_for_selector("div", state="attached", timeout=5000)
                await asyncio.sleep(3) # Espera extra de seguran√ßa para os dados popularem
            except:
                print("Aviso: Elemento espec√≠fico n√£o encontrado, tentando baixar mesmo assim.")

            content = await page.content()
            await browser.close()
            return content

    except Exception as e:
        return f"ERROR: {e}"

# --- 3. PARSER (REESCRITO PARA TABELAS) ---
def parser(html_content):
    from bs4 import BeautifulSoup
    
    if html_content and html_content.startswith("ERROR:"):
        return None, html_content

    soup = BeautifulSoup(html_content, 'html.parser')
    extracted_data_list = [] # Agora vamos retornar uma LISTA de linhas, n√£o s√≥ um dicion√°rio

    # --- L√ìGICA DE EXTRA√á√ÉO DA TABELA ---
    # Tenta encontrar todos os containers que parecem ser as linhas das casas de aposta.
    # Baseado na sua imagem, parece ser uma lista vertical.
    
    # ‚ö†Ô∏è AQUI √â ONDE VOC√ä PRECISA AJUSTAR OS SELETORES (CSS CLASSES) ‚ö†Ô∏è
    # Estou usando suposi√ß√µes comuns. Se n√£o funcionar, use o Inspecionar Elemento.
    
    # Estrat√©gia Gen√©rica: Procurar linhas que tenham n√∫meros decimais (odds)
    # Vamos tentar achar todos os elementos que cont√™m o nome das casas
    
    # Tentativa 1: Procurar divs ou trs (linhas da tabela)
    # Muitas vezes esses sites usam divs com classes tipo "row", "item", "bookmaker"
    rows = soup.find_all(['div', 'tr']) 
    
    current_page_title = soup.title.string if soup.title else "Sem T√≠tulo"

    for row in rows:
        row_text = row.get_text(" ", strip=True) # Pega todo texto da linha
        
        # Filtro simples: Se a linha tiver nomes conhecidos OU parecer ter odds
        # Isso ajuda a ignorar cabe√ßalhos ou rodap√©s in√∫teis
        if "Bet365" in row_text or "Betano" in row_text or "1.3" in row_text:
            
            # Tenta extrair peda√ßos espec√≠ficos
            # Precisamos identificar as colunas.
            
            item = {}
            item['titulo_jogo'] = current_page_title
            item['texto_completo_linha'] = row_text[:200] # Debug: ver o que ele pegou
            
            # TENTATIVA DE EXTRA√á√ÉO DE ODDS (L√≥gica baseada em posi√ß√£o ou padr√£o)
            # Vamos procurar padr√µes num√©ricos de odds (ex: 1.36, 5.00) dentro desta linha
            import re
            odds_encontradas = re.findall(r'\d+\.\d{2}', row_text)
            
            if len(odds_encontradas) >= 3:
                # Geralmente a ordem √©: Odd 1 (Casa), Odd X (Empate), Odd 2 (Fora)
                item['Odd_1'] = odds_encontradas[0]
                item['Odd_X'] = odds_encontradas[1]
                item['Odd_2'] = odds_encontradas[2]
                
                # O nome da casa geralmente √© a primeira palavra ou texto antes dos n√∫meros
                # Aqui √© dif√≠cil ser preciso sem o seletor exato, mas vamos tentar:
                item['Casa'] = "Identificar Manualmente" 
                if "Bet365" in row_text: item['Casa'] = "Bet365"
                elif "Betano" in row_text: item['Casa'] = "Betano"
                elif "1xBet" in row_text: item['Casa'] = "1xBet"
                elif "Sportingbet" in row_text: item['Casa'] = "Sportingbet"
                else: 
                     # Tenta pegar o in√≠cio da string como nome
                     item['Casa'] = row_text.split(' ')[0]

                extracted_data_list.append(item)

    # Remove duplicatas (comum ao raspar divs aninhadas)
    unique_data = []
    seen = set()
    for d in extracted_data_list:
        # Cria uma chave √∫nica baseada nas odds e casa para n√£o repetir
        key = f"{d.get('Casa')}-{d.get('Odd_1')}"
        if key not in seen and len(d.get('Odd_1', '')) > 0:
            seen.add(key)
            unique_data.append(d)

    return unique_data, None

# --- 4. PIPELINE (AJUSTE PARA LISTAS) ---
def pipeline(raw_data_list, url):
    # Como agora recebemos uma LISTA de dicion√°rios, e n√£o um s√≥ dicion√°rio
    if not raw_data_list:
        return []
        
    cleaned_list = []
    for data in raw_data_list:
        d = data.copy()
        d['url_origem'] = url
        d['status'] = "‚úÖ Coletado"
        cleaned_list.append(d)
        
    return cleaned_list

# --- FUN√á√ÉO PRINCIPAL DE ORQUESTRA√á√ÉO (ASYNC) ---
async def run_scraper_logic(urls, delay):
    results = []
    
    # Elementos de UI para atualiza√ß√£o em tempo real
    progress_bar = st.progress(0)
    status_text = st.empty()
    total_urls = len(urls)
    
    for i, url in enumerate(urls):
        status_text.text(f"Processando {i+1}/{total_urls}: {url}")
        
        # Chama o downloader ass√≠ncrono
        html_content = await downloader_async(url)
        
        if html_content:
            # Chama o parser (s√≠ncrono, pois √© s√≥ processamento de texto)
            raw_data, error_msg = parser(html_content)
            
            if error_msg:
                st.error(f"Falha na URL {url}: {error_msg}")
            elif raw_data: # raw_data agora √© uma lista de casas de aposta
                clean_list = pipeline(raw_data, url)
                results.extend(clean_list) # Use EXTEND, n√£o append, para juntar listas
        
        # Delay entre requisi√ß√µes (exceto na √∫ltima)
        if i < total_urls - 1:
            with st.spinner(f"‚è≥ Aguardando {delay}s..."):
                await asyncio.sleep(delay) # Importante: asyncio.sleep, n√£o time.sleep
        
        progress_bar.progress((i + 1) / total_urls)
        
    status_text.text("Conclu√≠do!")
    return results

# --- INTERFACE ---
st.sidebar.header("Configura√ß√µes")
delay_seconds = st.sidebar.number_input("Delay (segundos)", min_value=1, value=5) # Reduzi para 5s para testes

st.subheader("Insira as URLs")
urls_input = st.text_area("URLs:", height=150, value="https://example.com")

if st.button("üöÄ Iniciar Scraping"):
    urls_to_visit = scheduler(urls_input)
    
    if not urls_to_visit:
        st.warning("Insira URLs v√°lidas.")
    else:
        # PONTO M√ÅGICO: Executa o loop ass√≠ncrono dentro do Streamlit
        final_data = asyncio.run(run_scraper_logic(urls_to_visit, delay_seconds))
        
        if final_data:
            df = pd.DataFrame(final_data)
            st.success("Dados extra√≠dos com sucesso!")
            st.dataframe(df)
            
            csv = df.to_csv(index=False).encode('utf-8')
            st.download_button("üì• Baixar CSV", csv, "dados.csv", "text/csv")
        else:
            st.warning("Nenhum dado retornado.")